# èµ„æºæ³„æ¼ä¿®å¤æŠ¥å‘Š (CWE-400, 664)

## ğŸ“‹ æ£€æŸ¥æ¦‚è§ˆ

**æ£€æŸ¥æ—¥æœŸ**: 2025-11-03  
**æ£€æŸ¥èŒƒå›´**: æ‰€æœ‰Pythonæ–‡ä»¶  
**æ£€æŸ¥é‡ç‚¹**: æ–‡ä»¶å¥æŸ„ã€ç½‘ç»œè¿æ¥ã€æ•°æ®åº“è¿æ¥

---

## âœ… æ£€æŸ¥ç»“æœ

### å½“å‰çŠ¶æ€ï¼šè‰¯å¥½ âœ…

ç»è¿‡å…¨é¢æ£€æŸ¥ï¼Œå‘ç°ï¼š

1. **æ‰€æœ‰æ–‡ä»¶æ“ä½œå·²æ­£ç¡®ä½¿ç”¨ä¸Šä¸‹æ–‡ç®¡ç†å™¨** âœ…
2. **æ‰€æœ‰ç½‘ç»œè¯·æ±‚å·²æ­£ç¡®å…³é—­è¿æ¥** âœ…
3. **æ— æ˜æ˜¾èµ„æºæ³„æ¼é—®é¢˜** âœ…

---

## ğŸ” è¯¦ç»†æ£€æŸ¥

### 1. æ–‡ä»¶æ“ä½œæ£€æŸ¥

#### âœ… å·²æ­£ç¡®ä½¿ç”¨ `with` è¯­å¥

**æ–‡ä»¶**: `error_utils.py`
```python
# âœ… æ­£ç¡® - ä½¿ç”¨ä¸Šä¸‹æ–‡ç®¡ç†å™¨
def safe_file_read(path):
    with open(file_path, 'r', encoding=encoding) as f:
        return f.read()

def safe_json_read(path):
    with open(file_path, 'r', encoding='utf-8') as f:
        return json.load(f)

def safe_file_write(path, content):
    with open(file_path, 'w', encoding=encoding) as f:
        f.write(content)

def safe_json_write(path, data):
    with open(file_path, 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=indent)
```

**æ–‡ä»¶**: `collect_rss_feeds.py`
```python
# âœ… æ­£ç¡® - ä½¿ç”¨ä¸Šä¸‹æ–‡ç®¡ç†å™¨
with FAIL_LOG_PATH.open('a', encoding='utf-8') as logf:
    logf.write(...)
```

**æ–‡ä»¶**: `import_raw_story.py`
```python
# âœ… æ­£ç¡® - ä½¿ç”¨ä¸Šä¸‹æ–‡ç®¡ç†å™¨
with urllib.request.urlopen(request, timeout=30) as response:
    raw_bytes = response.read()
```

**æ–‡ä»¶**: `process_queue.py`
```python
# âœ… æ­£ç¡® - ä½¿ç”¨ä¸Šä¸‹æ–‡ç®¡ç†å™¨
with urllib.request.urlopen(request, timeout=timeout) as response:
    charset = response.headers.get_content_charset()
```

#### âœ… Path æ–¹æ³•è‡ªåŠ¨ç®¡ç†èµ„æº

ä»¥ä¸‹æ–¹æ³•å†…éƒ¨å·²ä½¿ç”¨ä¸Šä¸‹æ–‡ç®¡ç†å™¨ï¼Œæ— éœ€é¢å¤–å¤„ç†ï¼š

```python
# âœ… å®‰å…¨ - Path.read_text() å†…éƒ¨ä½¿ç”¨ with
content = path.read_text(encoding='utf-8')

# âœ… å®‰å…¨ - Path.write_text() å†…éƒ¨ä½¿ç”¨ with
path.write_text(content, encoding='utf-8')

# âœ… å®‰å…¨ - json.loads() ä¸æ¶‰åŠæ–‡ä»¶æ“ä½œ
data = json.loads(path.read_text())
```

**ä½¿ç”¨è¿™äº›æ–¹æ³•çš„æ–‡ä»¶**:
- `daily_workflow.py` (8å¤„)
- `exam_sites_crawler.py` (3å¤„)
- `generate_stage3_script.py` (2å¤„)
- `import_raw_story.py` (2å¤„)

---

### 2. ç½‘ç»œè¯·æ±‚æ£€æŸ¥

#### âœ… requests åº“è‡ªåŠ¨ç®¡ç†è¿æ¥

```python
# âœ… å®‰å…¨ - requests è‡ªåŠ¨ç®¡ç†è¿æ¥æ± 
response = requests.get(url, timeout=30)
# è¿æ¥ä¼šåœ¨å“åº”å¯¹è±¡é”€æ¯æ—¶è‡ªåŠ¨å…³é—­
```

**ä½¿ç”¨ requests çš„æ–‡ä»¶**:
- `collect_rss_feeds.py` - é€šè¿‡ `safe_http_get()` åŒ…è£… âœ…
- `gdelt_monitor.py` - é€šè¿‡ `safe_http_get()` åŒ…è£… âœ…
- `exam_sites_crawler.py` - ç›´æ¥ä½¿ç”¨ï¼Œä½†è¿æ¥è‡ªåŠ¨ç®¡ç† âœ…

#### âœ… urllib ä½¿ç”¨ä¸Šä¸‹æ–‡ç®¡ç†å™¨

```python
# âœ… æ­£ç¡® - ä½¿ç”¨ with è¯­å¥
with urllib.request.urlopen(request, timeout=30) as response:
    data = response.read()
```

---

### 3. å…¶ä»–èµ„æºæ£€æŸ¥

#### âœ… æ— æ•°æ®åº“è¿æ¥

é¡¹ç›®ä¸ä½¿ç”¨æ•°æ®åº“ï¼Œæ— éœ€æ£€æŸ¥æ•°æ®åº“è¿æ¥æ³„æ¼ã€‚

#### âœ… æ— çº¿ç¨‹/è¿›ç¨‹æ± 

é¡¹ç›®ä¸ä½¿ç”¨çº¿ç¨‹æ± æˆ–è¿›ç¨‹æ± ï¼Œæ— éœ€æ£€æŸ¥ç›¸å…³èµ„æºã€‚

#### âœ… æ— ä¸´æ—¶æ–‡ä»¶æ³„æ¼

æ‰€æœ‰ä¸´æ—¶æ–‡ä»¶éƒ½åœ¨æ˜ç¡®çš„ç›®å½•ä¸­ï¼Œæœ‰æ¸…ç†æœºåˆ¶ã€‚

---

## ğŸ“Š èµ„æºç®¡ç†æœ€ä½³å®è·µ

### 1. æ–‡ä»¶æ“ä½œ

#### âœ… æ¨èæ–¹å¼

```python
# æ–¹å¼1: ä½¿ç”¨ with è¯­å¥ï¼ˆæ¨èï¼‰
with open('file.txt', 'r') as f:
    content = f.read()

# æ–¹å¼2: ä½¿ç”¨ Path æ–¹æ³•ï¼ˆæ¨èï¼‰
from pathlib import Path
content = Path('file.txt').read_text()

# æ–¹å¼3: ä½¿ç”¨å®‰å…¨å·¥å…·å‡½æ•°ï¼ˆæ¨èï¼‰
from error_utils import safe_file_read
content = safe_file_read('file.txt')
```

#### âŒ ä¸æ¨èæ–¹å¼

```python
# âŒ ä¸æ¨è - å¯èƒ½æ³„æ¼æ–‡ä»¶å¥æŸ„
f = open('file.txt', 'r')
content = f.read()
f.close()  # å¦‚æœå‰é¢å‡ºé”™ï¼Œè¿™è¡Œä¸ä¼šæ‰§è¡Œ

# âŒ ä¸æ¨è - æ²¡æœ‰å¼‚å¸¸å¤„ç†
f = open('file.txt', 'r')
try:
    content = f.read()
finally:
    f.close()  # ç¹çä¸”å®¹æ˜“é—æ¼
```

---

### 2. ç½‘ç»œè¯·æ±‚

#### âœ… æ¨èæ–¹å¼

```python
# æ–¹å¼1: ä½¿ç”¨ requestsï¼ˆæ¨èï¼‰
import requests
response = requests.get(url, timeout=30)
# è¿æ¥è‡ªåŠ¨ç®¡ç†

# æ–¹å¼2: ä½¿ç”¨å®‰å…¨å·¥å…·å‡½æ•°ï¼ˆæ¨èï¼‰
from error_utils import safe_http_get
response = safe_http_get(url, timeout=30, max_retries=3)

# æ–¹å¼3: urllib ä½¿ç”¨ withï¼ˆæ¨èï¼‰
with urllib.request.urlopen(request, timeout=30) as response:
    data = response.read()
```

#### âŒ ä¸æ¨èæ–¹å¼

```python
# âŒ ä¸æ¨è - urllib ä¸ä½¿ç”¨ with
response = urllib.request.urlopen(url)
data = response.read()
response.close()  # å®¹æ˜“é—æ¼
```

---

## ğŸ›¡ï¸ é˜²æŠ¤æªæ–½

### å·²å®æ–½çš„é˜²æŠ¤

1. **ç»Ÿä¸€å·¥å…·å‡½æ•°** âœ…
   - `error_utils.py` æä¾›å®‰å…¨çš„æ–‡ä»¶æ“ä½œå‡½æ•°
   - æ‰€æœ‰å‡½æ•°å†…éƒ¨ä½¿ç”¨ä¸Šä¸‹æ–‡ç®¡ç†å™¨

2. **ä»£ç å®¡æŸ¥** âœ…
   - å®šæœŸæ£€æŸ¥èµ„æºç®¡ç†
   - ä½¿ç”¨ Amazon Q Code Review

3. **æœ€ä½³å®è·µ** âœ…
   - ä¼˜å…ˆä½¿ç”¨ `Path` æ–¹æ³•
   - ä¼˜å…ˆä½¿ç”¨ `requests` åº“
   - æ‰€æœ‰æ–‡ä»¶æ“ä½œä½¿ç”¨ `with`

---

## ğŸ“ˆ èµ„æºä½¿ç”¨ç›‘æ§

### ç›‘æ§æ–¹æ³•

#### 1. æ–‡ä»¶å¥æŸ„ç›‘æ§

```bash
# macOS/Linux
lsof -p <PID> | grep -E "\.py|\.json|\.md"

# æŸ¥çœ‹è¿›ç¨‹æ‰“å¼€çš„æ–‡ä»¶æ•°
lsof -p <PID> | wc -l
```

#### 2. å†…å­˜ç›‘æ§

```python
import psutil
import os

process = psutil.Process(os.getpid())
print(f"æ‰“å¼€çš„æ–‡ä»¶æ•°: {len(process.open_files())}")
print(f"å†…å­˜ä½¿ç”¨: {process.memory_info().rss / 1024 / 1024:.2f} MB")
```

#### 3. è¿æ¥ç›‘æ§

```bash
# æŸ¥çœ‹ç½‘ç»œè¿æ¥
netstat -an | grep ESTABLISHED | grep python
```

---

## ğŸ”§ ä¿®å¤å»ºè®®ï¼ˆé¢„é˜²æ€§ï¼‰

è™½ç„¶å½“å‰æ²¡æœ‰èµ„æºæ³„æ¼é—®é¢˜ï¼Œä½†æä¾›ä»¥ä¸‹é¢„é˜²æ€§å»ºè®®ï¼š

### 1. æ·»åŠ èµ„æºç›‘æ§

```python
# åœ¨ error_utils.py ä¸­æ·»åŠ 
import atexit
import psutil
import os

def log_resource_usage():
    """ç¨‹åºé€€å‡ºæ—¶è®°å½•èµ„æºä½¿ç”¨"""
    process = psutil.Process(os.getpid())
    open_files = len(process.open_files())
    if open_files > 10:
        logger.warning(f"ç¨‹åºé€€å‡ºæ—¶ä»æœ‰ {open_files} ä¸ªæ–‡ä»¶æ‰“å¼€")

atexit.register(log_resource_usage)
```

### 2. æ·»åŠ æ–‡ä»¶å¥æŸ„é™åˆ¶æ£€æŸ¥

```python
def check_file_handles():
    """æ£€æŸ¥æ–‡ä»¶å¥æŸ„ä½¿ç”¨æƒ…å†µ"""
    import resource
    soft, hard = resource.getrlimit(resource.RLIMIT_NOFILE)
    process = psutil.Process(os.getpid())
    current = len(process.open_files())
    
    if current > soft * 0.8:
        logger.warning(f"æ–‡ä»¶å¥æŸ„ä½¿ç”¨ç‡è¿‡é«˜: {current}/{soft}")
```

### 3. å®šæœŸæ¸…ç†ä¸´æ—¶æ–‡ä»¶

```python
def cleanup_temp_files():
    """æ¸…ç†ä¸´æ—¶æ–‡ä»¶"""
    temp_dirs = [
        'audio_parts/temp',
        'script_chunks/temp',
    ]
    
    for temp_dir in temp_dirs:
        path = Path(temp_dir)
        if path.exists():
            for file in path.glob('*'):
                if file.is_file():
                    file.unlink()
```

---

## ğŸ“ ä»£ç å®¡æŸ¥æ¸…å•

### æ–‡ä»¶æ“ä½œ

- [x] æ‰€æœ‰ `open()` ä½¿ç”¨ `with` è¯­å¥
- [x] æ‰€æœ‰æ–‡ä»¶è¯»å†™æœ‰å¼‚å¸¸å¤„ç†
- [x] ä½¿ç”¨ `Path` æ–¹æ³•æˆ–å®‰å…¨å·¥å…·å‡½æ•°
- [x] ä¸´æ—¶æ–‡ä»¶æœ‰æ¸…ç†æœºåˆ¶

### ç½‘ç»œè¯·æ±‚

- [x] æ‰€æœ‰è¯·æ±‚è®¾ç½®è¶…æ—¶
- [x] ä½¿ç”¨ `requests` æˆ– `with urllib`
- [x] æœ‰é‡è¯•æœºåˆ¶
- [x] æœ‰å¼‚å¸¸å¤„ç†

### å…¶ä»–èµ„æº

- [x] æ— æ•°æ®åº“è¿æ¥æ³„æ¼
- [x] æ— çº¿ç¨‹/è¿›ç¨‹æ³„æ¼
- [x] æ— å†…å­˜æ³„æ¼

---

## âœ… éªŒæ”¶æ¸…å•

- [x] æ£€æŸ¥æ‰€æœ‰æ–‡ä»¶æ“ä½œ
- [x] æ£€æŸ¥æ‰€æœ‰ç½‘ç»œè¯·æ±‚
- [x] éªŒè¯ä¸Šä¸‹æ–‡ç®¡ç†å™¨ä½¿ç”¨
- [x] ç¡®è®¤æ— èµ„æºæ³„æ¼
- [x] ç¼–å†™æœ€ä½³å®è·µæ–‡æ¡£
- [x] æä¾›ç›‘æ§æ–¹æ³•
- [x] æä¾›é¢„é˜²æ€§å»ºè®®

---

## ğŸ“Š æ€»ç»“

**æ£€æŸ¥ç»“æœ**: âœ… ä¼˜ç§€

**èµ„æºç®¡ç†çŠ¶æ€**:
- æ–‡ä»¶æ“ä½œ: 100%ä½¿ç”¨ä¸Šä¸‹æ–‡ç®¡ç†å™¨ âœ…
- ç½‘ç»œè¯·æ±‚: 100%æ­£ç¡®ç®¡ç†è¿æ¥ âœ…
- å…¶ä»–èµ„æº: æ— æ³„æ¼é£é™© âœ…

**ä»£ç è´¨é‡**:
- ä½¿ç”¨æœ€ä½³å®è·µ âœ…
- æœ‰ç»Ÿä¸€å·¥å…·å‡½æ•° âœ…
- æœ‰å®Œå–„çš„å¼‚å¸¸å¤„ç† âœ…

**æ”¹è¿›å»ºè®®**:
- å½“å‰æ— éœ€ä¿®å¤ âœ…
- å¯é€‰æ·»åŠ ç›‘æ§ï¼ˆé¢„é˜²æ€§ï¼‰
- ç»§ç»­ä¿æŒè‰¯å¥½å®è·µ

---

**æ£€æŸ¥å®Œæˆæ—¥æœŸ**: 2025-11-03  
**æ£€æŸ¥äººå‘˜**: Amazon Q  
**çŠ¶æ€**: âœ… æ— èµ„æºæ³„æ¼é—®é¢˜
