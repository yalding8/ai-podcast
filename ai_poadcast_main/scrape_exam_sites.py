#!/usr/bin/env python3
"""
è€ƒè¯•æœºæ„ç½‘ç«™çˆ¬è™«
é’ˆå¯¹æ²¡æœ‰RSSçš„å®˜ç½‘ï¼ˆIELTSã€TOEFLç­‰ï¼‰
"""

import requests
from bs4 import BeautifulSoup
from datetime import datetime, timezone
import json
from pathlib import Path

EXAM_SITES = {
    'IELTS': {
        'url': 'https://ielts.org/news-and-insights',
        'selector': '.news-item',  # éœ€è¦æ ¹æ®å®é™…é¡µé¢è°ƒæ•´
        'title_selector': '.news-title',
        'link_selector': 'a',
        'date_selector': '.news-date'
    },
    'TOEFL': {
        'url': 'https://www.ets.org/toefl/test-takers/ibt/news.html',
        'selector': '.news-article',
        'title_selector': 'h3',
        'link_selector': 'a',
        'date_selector': '.date'
    }
}

def scrape_site(site_name, config):
    """çˆ¬å–å•ä¸ªç½‘ç«™"""
    print(f"\nğŸ•·ï¸  æ­£åœ¨çˆ¬å–: {site_name}")
    
    try:
        response = requests.get(config['url'], timeout=10, headers={
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36'
        })
        response.raise_for_status()
    except Exception as e:
        print(f"  âŒ è¯·æ±‚å¤±è´¥: {e}")
        return []
    
    soup = BeautifulSoup(response.text, 'html.parser')
    items = []
    
    # æ ¹æ®é…ç½®æå–
    for article in soup.select(config['selector'])[:10]:  # æœ€å¤šå–10æ¡
        try:
            title_elem = article.select_one(config['title_selector'])
            link_elem = article.select_one(config['link_selector'])
            
            if not title_elem or not link_elem:
                continue
            
            title = title_elem.get_text(strip=True)
            link = link_elem['href']
            
            # å¤„ç†ç›¸å¯¹é“¾æ¥
            if not link.startswith('http'):
                from urllib.parse import urljoin
                link = urljoin(config['url'], link)
            
            # æ—¥æœŸï¼ˆå¯é€‰ï¼‰
            date = ''
            if config.get('date_selector'):
                date_elem = article.select_one(config['date_selector'])
                if date_elem:
                    date = date_elem.get_text(strip=True)
            
            items.append({
                'title': title,
                'url': link,
                'source': site_name,
                'published': date,
                'tags': ['exam', site_name.lower()],
                'priority': 9,
                'collected_at': datetime.now(timezone.utc).isoformat()
            })
        
        except Exception as e:
            print(f"  âš ï¸  è§£ææ¡ç›®å¤±è´¥: {e}")
            continue
    
    print(f"  âœ… å‘ç° {len(items)} æ¡æ›´æ–°")
    return items

def main():
    """ä¸»æµç¨‹"""
    all_items = []
    
    for site_name, config in EXAM_SITES.items():
        items = scrape_site(site_name, config)
        all_items.extend(items)
    
    if all_items:
        # è¿½åŠ åˆ°é˜Ÿåˆ—
        queue_file = Path("ai_poadcast_main/news_queue.json")
        
        if queue_file.exists():
            with open(queue_file) as f:
                queue = json.load(f)
        else:
            queue = {'items': []}
        
        queue['items'].extend(all_items)
        queue['updated_at'] = datetime.now(timezone.utc).isoformat()
        queue['total'] = len(queue['items'])
        
        with open(queue_file, 'w', encoding='utf-8') as f:
            json.dump(queue, f, ensure_ascii=False, indent=2)
        
        print(f"\nâœ… å·²æ·»åŠ  {len(all_items)} æ¡åˆ°é˜Ÿåˆ—")
    else:
        print("\nâš ï¸  æœªå‘ç°æ–°å†…å®¹")

if __name__ == "__main__":
    main()