#!/usr/bin/env python3
"""
æ–°é—»é˜Ÿåˆ—å¤„ç†å™¨ - è°ƒè¯•ç‰ˆæœ¬
"""

import json
import os
import re
import subprocess
import urllib.error
import urllib.request
from datetime import date, datetime, timezone
from email.utils import parsedate_to_datetime
from pathlib import Path
from typing import Optional

try:
    from openai import OpenAI
except ImportError:
    OpenAI = None

try:
    import anthropic
except ImportError:
    anthropic = None

try:
    import requests
except ImportError:
    requests = None


def init_translator(provider: str, model: str):
    """åˆå§‹åŒ–ç¿»è¯‘å®¢æˆ·ç«¯"""
    if provider == 'openai':
        if OpenAI is None:
            print("âš ï¸ æœªå®‰è£… openai åº“ï¼Œæ— æ³•å¯ç”¨æ‘˜è¦ç¿»è¯‘ã€‚")
            return None
        try:
            client = OpenAI()
        except Exception as exc:  # pragma: no cover
            print(f"âš ï¸ æ— æ³•åˆå§‹åŒ– OpenAI å®¢æˆ·ç«¯ï¼š{exc}")
            return None
    elif provider == 'anthropic':
        if anthropic is None:
            print("âš ï¸ æœªå®‰è£… anthropic åº“ï¼Œæ— æ³•å¯ç”¨æ‘˜è¦ç¿»è¯‘ã€‚")
            return None
        try:
            client = anthropic.Anthropic()
        except Exception as exc:  # pragma: no cover
            print(f"âš ï¸ æ— æ³•åˆå§‹åŒ– Anthropic å®¢æˆ·ç«¯ï¼š{exc}")
            return None
    else:
        print(f"âš ï¸ æœªçŸ¥ç¿»è¯‘æä¾›æ–¹: {provider}")
        return None
    
    return {
        'provider': provider,
        'client': client,
        'model': model,
        'cache': {},
        'disabled': False
    }


def translate_summary_to_zh(summary: str, translator: Optional[dict]):
    """è°ƒç”¨æŒ‡å®šæ¨¡å‹å°†è‹±æ–‡æ‘˜è¦ç¿»è¯‘æˆä¸­æ–‡"""
    if not translator or translator.get('disabled'):
        return None
    if not summary:
        return ""
    summary = summary.strip()
    if not summary:
        return ""
    cache = translator['cache']
    if summary in cache:
        return cache[summary]
    
    prompt = (
        "è¯·å°†ä»¥ä¸‹è‹±æ–‡æ–°é—»æ‘˜è¦ç¿»è¯‘æˆè‡ªç„¶ã€å‡†ç¡®çš„ç®€ä½“ä¸­æ–‡ï¼Œä¿ç•™ä¸“æœ‰åè¯ï¼Œä¸æ·»åŠ é¢å¤–è¯´æ˜ï¼š\n\n"
        f"{summary}"
    )
    
    provider = translator.get('provider', 'openai')
    
    try:
        if provider == 'openai':
            response = translator['client'].chat.completions.create(
                model=translator['model'],
                messages=[
                    {"role": "system", "content": "You are a professional English-to-Chinese news translator."},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.2,
            )
            translation = response.choices[0].message.content.strip()
        elif provider == 'anthropic':
            response = translator['client'].messages.create(
                model=translator['model'],
                system="You are a professional English-to-Chinese news translator.",
                messages=[
                    {"role": "user", "content": prompt}
                ],
                max_tokens=512,
                temperature=0.2
            )
            translation_parts = []
            for block in response.content:
                text = getattr(block, "text", None)
                if text:
                    translation_parts.append(text.strip())
            translation = "\n".join(part for part in translation_parts if part)
        else:
            print(f"  âš ï¸ æœªçŸ¥ç¿»è¯‘æä¾›æ–¹: {provider}ï¼Œå·²åœç”¨ç¿»è¯‘ã€‚")
            translator['disabled'] = True
            cache[summary] = None
            return None
        
        translation = translation.strip()
        cache[summary] = translation
        return translation
    except Exception as exc:  # pragma: no cover
        error_text = str(exc)
        quota_indicators = (
            'insufficient_quota',
            'exceeded your current quota',
            'rate limit',
            'RateLimit',
            '429'
        )
        auth_indicators = (
            'authentication_error',
            'invalid x-api-key',
            'Invalid API key',
            'invalid_api_key',
            'API key is missing',
            'PermissionDenied',
            'Could not resolve authentication method',
            'Unauthorized',
            'invalid authentication',
        )
        if any(indicator in error_text for indicator in quota_indicators):
            print("  âš ï¸ æ‘˜è¦ç¿»è¯‘å¤±è´¥ï¼šé¢åº¦æˆ–é€Ÿç‡å—é™ï¼Œå·²åœç”¨ç¿»è¯‘ã€‚")
            translator['disabled'] = True
        elif any(indicator in error_text for indicator in auth_indicators):
            print("  âš ï¸ æ‘˜è¦ç¿»è¯‘å¤±è´¥ï¼šAPI Key éªŒè¯å¤±è´¥ï¼Œå·²åœç”¨ç¿»è¯‘ã€‚")
            translator['disabled'] = True
        else:
            print(f"  âš ï¸ æ‘˜è¦ç¿»è¯‘å¤±è´¥ï¼š{exc}")
        cache[summary] = None
        return None


DEFAULT_USER_AGENT = (
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 14_0) AppleWebKit/537.36 "
    "(KHTML, like Gecko) Chrome/118.0 Safari/537.36"
)
_LLM_CLIENT_CACHE = {}
_LLM_DISABLED = {}
_LLM_DISABLED_NOTIFIED = set()


def _fallback_extract_text(html: str) -> str:
    cleaned = re.sub(r"(?is)<(script|style|noscript).*?>.*?</\1>", "", html)
    cleaned = re.sub(r"(?s)<[^>]+>", "\n", cleaned)
    lines = [line.strip() for line in cleaned.splitlines()]
    return "\n".join(line for line in lines if line)


def _extract_iso_date(raw: Optional[str]) -> Optional[str]:
    if not raw:
        return None
    text = raw.strip()
    if not text:
        return None

    match = re.search(r"(\d{4}-\d{2}-\d{2})", text)
    if match:
        return match.group(1)

    try:
        dt = datetime.fromisoformat(text)
    except ValueError:
        dt = None
    if dt:
        return dt.date().isoformat()

    try:
        dt = parsedate_to_datetime(text)
    except (TypeError, ValueError):
        dt = None
    if dt:
        return dt.date().isoformat()

    date_patterns = [
        "%Y/%m/%d",
        "%d %b %Y",
        "%d %B %Y",
        "%b %d, %Y",
        "%B %d, %Y",
        "%m/%d/%Y",
        "%d-%b-%Y",
    ]
    for pattern in date_patterns:
        try:
            dt = datetime.strptime(text, pattern)
            return dt.date().isoformat()
        except ValueError:
            continue

    return None


def _validate_iso_date(value: str) -> Optional[str]:
    try:
        dt = datetime.strptime(value, "%Y-%m-%d")
        return dt.date().isoformat()
    except ValueError:
        return None


def fetch_article_text(url: str, user_agent: Optional[str] = None, timeout: int = 30) -> Optional[str]:
    """æŠ“å–ç½‘é¡µæ­£æ–‡å¹¶åšåŸºæœ¬æ¸…æ´—"""
    if not url:
        print("  âš ï¸ æœªæä¾› URLï¼Œæ— æ³•æŠ“å–æ­£æ–‡ã€‚")
        return None

    ua = user_agent or os.getenv("NEWS_FETCH_USER_AGENT") or DEFAULT_USER_AGENT
    request = urllib.request.Request(url, headers={"User-Agent": ua})

    try:
        with urllib.request.urlopen(request, timeout=timeout) as response:
            charset = response.headers.get_content_charset() or "utf-8"
            raw_bytes = response.read()
    except urllib.error.HTTPError as exc:
        print(f"  âš ï¸ æŠ“å–å¤±è´¥ï¼ˆHTTP {exc.code}ï¼‰ï¼š{exc.reason}")
        return None
    except urllib.error.URLError as exc:
        print(f"  âš ï¸ æŠ“å–å¤±è´¥ï¼š{exc.reason}")
        return None
    except Exception as exc:  # pragma: no cover
        print(f"  âš ï¸ æŠ“å–å¤±è´¥ï¼š{exc}")
        return None

    raw_html = raw_bytes.decode(charset, errors="replace")

    try:
        from import_raw_story import extract_readable_text
    except ImportError:
        text = _fallback_extract_text(raw_html)
    else:
        try:
            text = extract_readable_text(raw_html)
        except Exception as exc:  # pragma: no cover
            print(f"  âš ï¸ è§£ææ­£æ–‡å¤±è´¥ï¼š{exc}")
            text = _fallback_extract_text(raw_html)

    cleaned = (text or "").strip()
    if not cleaned:
        print("  âš ï¸ æˆåŠŸæŠ“å–é¡µé¢ï¼Œä½†æœªèƒ½è§£ææ­£æ–‡ã€‚")
        return None
    return cleaned


def _get_llm_client(provider: str):
    provider = provider.lower()
    if provider in _LLM_DISABLED:
        return None
    if provider in _LLM_CLIENT_CACHE:
        return _LLM_CLIENT_CACHE[provider]

    client = None
    if provider == 'openai':
        if OpenAI is None:
            print("  âš ï¸ æœªå®‰è£… openai åº“ï¼Œæ— æ³•ç”Ÿæˆè¦ç‚¹ã€‚")
        else:
            try:
                client = OpenAI()
            except Exception as exc:  # pragma: no cover
                print(f"  âš ï¸ æ— æ³•åˆå§‹åŒ– OpenAI å®¢æˆ·ç«¯ï¼š{exc}")
    elif provider == 'anthropic':
        if anthropic is None:
            print("  âš ï¸ æœªå®‰è£… anthropic åº“ï¼Œæ— æ³•ç”Ÿæˆè¦ç‚¹ã€‚")
        else:
            try:
                client = anthropic.Anthropic()
            except Exception as exc:  # pragma: no cover
                print(f"  âš ï¸ æ— æ³•åˆå§‹åŒ– Anthropic å®¢æˆ·ç«¯ï¼š{exc}")
    elif provider == 'deepseek':
        api_key = os.getenv("DEEPSEEK_API_KEY") or os.getenv("DEEPSEEK_KEY")
        if not api_key:
            print("  âš ï¸ æœªè®¾ç½® DEEPSEEK_API_KEYï¼Œæ— æ³•ç”Ÿæˆè¦ç‚¹ã€‚")
        elif requests is None:
            print("  âš ï¸ æœªå®‰è£… requests åº“ï¼Œæ— æ³•è°ƒç”¨ DeepSeek æ¨¡å‹ã€‚")
        else:
            client = {
                "api_key": api_key,
                "base_url": os.getenv("DEEPSEEK_API_BASE", "https://api.deepseek.com"),
            }
    else:
        print(f"  âš ï¸ æœªçŸ¥ç”Ÿæˆæ¨¡å‹æä¾›æ–¹: {provider}")

    _LLM_CLIENT_CACHE[provider] = client
    return client


def _invoke_llm(provider: str, model: str, system_prompt: str, user_prompt: str,
                temperature: float = 0.2, max_tokens: int = 512) -> Optional[str]:
    provider = provider.lower()
    if provider in _LLM_DISABLED:
        return None

    client = _get_llm_client(provider)
    if client is None:
        return None

    try:
        if provider == 'openai':
            response = client.chat.completions.create(
                model=model,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                ],
                temperature=temperature,
                max_tokens=max_tokens,
            )
            return response.choices[0].message.content.strip()
        elif provider == 'anthropic':
            response = client.messages.create(
                model=model,
                system=system_prompt,
                messages=[{"role": "user", "content": user_prompt}],
                temperature=temperature,
                max_tokens=max_tokens,
            )
            parts = []
            for block in response.content:
                text = getattr(block, "text", None)
                if text:
                    parts.append(text.strip())
            return "\n".join(part for part in parts if part).strip()
        elif provider == 'deepseek':
            if requests is None:
                raise RuntimeError("requests åº“æœªå®‰è£…ã€‚")
            if not isinstance(client, dict):
                raise RuntimeError("DeepSeek å®¢æˆ·ç«¯é…ç½®ç¼ºå¤±ã€‚")
            api_key = client.get("api_key")
            base_url = client.get("base_url", "https://api.deepseek.com").rstrip("/")
            if not api_key:
                raise RuntimeError("ç¼ºå°‘ DeepSeek API Keyã€‚")

            endpoint = f"{base_url}/v1/chat/completions"
            payload = {
                "model": model,
                "messages": [
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt},
                ],
                "temperature": temperature,
                "max_tokens": max_tokens,
            }
            headers = {
                "Authorization": f"Bearer {api_key}",
                "Content-Type": "application/json",
            }
            try:
                resp = requests.post(endpoint, json=payload, headers=headers, timeout=45)
            except Exception as exc:  # pragma: no cover
                raise RuntimeError(f"DeepSeek è¯·æ±‚å¤±è´¥ï¼š{exc}") from exc

            if resp.status_code >= 400:
                try:
                    error_body = resp.json()
                except ValueError:
                    error_body = resp.text[:200]
                raise RuntimeError(f"HTTP {resp.status_code}: {error_body}")

            try:
                data = resp.json()
            except ValueError as exc:  # pragma: no cover
                raise RuntimeError(f"DeepSeek è¿”å›é JSONï¼š{resp.text[:200]}") from exc

            choices = data.get("choices") or []
            if not choices:
                raise RuntimeError("DeepSeek è¿”å›ä¸­ç¼ºå°‘ choicesã€‚")
            message = choices[0].get("message") or {}
            content = message.get("content", "").strip()
            if not content:
                raise RuntimeError("DeepSeek è¿”å›å†…å®¹ä¸ºç©ºã€‚")
            return content
    except Exception as exc:  # pragma: no cover
        error_text = str(exc)
        print(f"  âš ï¸ ç”Ÿæˆè¦ç‚¹å¤±è´¥ï¼š{error_text}")

        quota_indicators = (
            'insufficient_quota',
            'exceeded your current quota',
            'rate limit',
            'RateLimit',
            '429'
        )
        auth_indicators = (
            'authentication_error',
            'invalid x-api-key',
            'Invalid API key',
            'invalid_api_key',
            'API key is missing',
            'PermissionDenied',
            'Could not resolve authentication method',
            'Unauthorized',
            'invalid authentication',
        )
        disable_reason = None
        if any(indicator in error_text for indicator in quota_indicators):
            disable_reason = "é¢åº¦æˆ–é€Ÿç‡å—é™ï¼Œè¯·æ£€æŸ¥è´¦å·æˆ–åˆ‡æ¢æ¨¡å‹ã€‚"
        elif any(indicator in error_text for indicator in auth_indicators):
            disable_reason = "API Key éªŒè¯å¤±è´¥ï¼Œè¯·æ›´æ–°å¯†é’¥åå†è¯•ã€‚"

        if disable_reason:
            _LLM_DISABLED[provider] = disable_reason
            _LLM_DISABLED_NOTIFIED.discard(provider)
            _LLM_CLIENT_CACHE.pop(provider, None)
        else:
            if provider not in _LLM_DISABLED_NOTIFIED:
                print(f"  âš ï¸ {provider} æ¨¡å‹å·²åœç”¨ï¼Œè¯·æ£€æŸ¥é…ç½®ã€‚")
                _LLM_DISABLED_NOTIFIED.add(provider)

        return None

    return None


def _fallback_summary(article_text: str, reason: Optional[str] = None) -> str:
    preview = re.split(r"(?<=[.!?ã€‚ï¼ï¼Ÿ])\s+", article_text.strip())
    snippet = " ".join(preview[:3]).strip()
    if not snippet:
        snippet = article_text[:240].strip()
    if reason:
        header = f"âš ï¸ è‡ªåŠ¨æç‚¼åœç”¨ï¼š{reason}"
    else:
        header = "âš ï¸ è‡ªåŠ¨æç‚¼å¤±è´¥ï¼Œè¯·æ‰‹åŠ¨å¤„ç†ã€‚"
    return (
        f"{header}\n\n"
        "è‹±æ–‡åŸæ–‡æ‘˜å½•ï¼š\n"
        f"{snippet}"
    )


def extract_key_points(title: str, url: str, article_text: str,
                       provider: Optional[str] = None,
                       model: Optional[str] = None) -> str:
    """ä½¿ç”¨å¤§æ¨¡å‹ç”Ÿæˆä¸­æ–‡è¦ç‚¹æ‘˜è¦ï¼Œå¤±è´¥æ—¶è¿”å›é™çº§æ–‡æ¡ˆ"""
    if not article_text:
        return "âš ï¸ æ­£æ–‡æŠ“å–å¤±è´¥ï¼Œæ— æ³•ç”Ÿæˆè¦ç‚¹ã€‚"

    provider_choice = (provider or os.getenv("KEYPOINT_PROVIDER") or "openai").lower()
    env_model = os.getenv("KEYPOINT_MODEL")
    if model:
        model_choice = model
    elif env_model:
        model_choice = env_model
    else:
        default_models = {
            "openai": "gpt-4o-mini",
            "anthropic": "claude-3-haiku-20240307",
            "deepseek": "deepseek-chat",
        }
        model_choice = default_models.get(provider_choice, "gpt-4o-mini")

    try:
        max_source_chars = int(os.getenv("KEYPOINT_MAX_SOURCE_CHARS", "6000"))
    except ValueError:
        max_source_chars = 6000

    try:
        max_tokens = int(os.getenv("KEYPOINT_MAX_TOKENS", "512"))
    except ValueError:
        max_tokens = 512

    truncated = article_text[:max_source_chars]
    if len(article_text) > max_source_chars:
        truncated += f"\n\n[åŸæ–‡æˆªæ–­ï¼šä¿ç•™å‰ {max_source_chars} å­—ï¼Œå…± {len(article_text)} å­—]"

    system_prompt = "You are a bilingual international education analyst. Produce structured Chinese key points."
    user_prompt = (
        "è¯·é˜…è¯»ä»¥ä¸‹è‹±æ–‡æ–°é—»ï¼Œå¹¶ç”¨ä¸“ä¸šçš„ç®€ä½“ä¸­æ–‡æ€»ç»“ 3-5 æ¡è¦ç‚¹ï¼Œæ¯æ¡ 25 å­—ä»¥å†…ã€‚"
        "è¾“å‡ºæ ¼å¼ç¤ºä¾‹ï¼š\n"
        "1. ...\n2. ...\n3. ...\n"
        "éœ€è¦†ç›–äº‹ä»¶èƒŒæ™¯ã€å—å½±å“äººç¾¤ã€æªæ–½/æ—¶é—´èŠ‚ç‚¹ï¼Œä»¥åŠç»™å›½é™…å­¦ç”Ÿçš„å»ºè®®ã€‚\n\n"
        f"æ–°é—»æ ‡é¢˜ï¼š{title}\n"
        f"åŸæ–‡é“¾æ¥ï¼š{url}\n\n"
        "è‹±æ–‡åŸæ–‡ï¼š\n"
        f"{truncated}"
    )

    summary = _invoke_llm(provider_choice, model_choice, system_prompt, user_prompt, max_tokens=max_tokens)
    if summary:
        return summary.strip()

    reason = _LLM_DISABLED.get(provider_choice)
    return _fallback_summary(article_text, reason)


def load_skipped_urls():
    """åŠ è½½å·²è·³è¿‡çš„URLåˆ—è¡¨"""
    from path_utils import safe_path
    from error_utils import safe_json_read
    
    skipped_file = safe_path("ai_poadcast_main/.skipped_urls.json", Path.cwd())
    data = safe_json_read(skipped_file, default={})
    
    if isinstance(data, dict):
        return set(data.get('urls', []))
    return set()

def save_skipped_url(url: str):
    """ä¿å­˜è·³è¿‡çš„URL"""
    from path_utils import safe_path
    from error_utils import safe_json_write
    
    skipped_file = safe_path("ai_poadcast_main/.skipped_urls.json", Path.cwd())
    skipped_urls = load_skipped_urls()
    skipped_urls.add(url)
    
    data = {
        'updated_at': datetime.now(timezone.utc).isoformat(),
        'urls': sorted(list(skipped_urls))
    }
    
    safe_json_write(skipped_file, data)

def load_queue(queue_file="ai_poadcast_main/news_queue.json",
               summaries_file: Optional[str] = None):
    """åŠ è½½é˜Ÿåˆ—"""
    from path_utils import safe_path
    from error_utils import safe_json_read
    
    queue_path = safe_path(queue_file, Path.cwd())
    
    print(f"[DEBUG] å°è¯•åŠ è½½é˜Ÿåˆ—: {queue_file}")
    
    data = safe_json_read(queue_path, default={'items': []})
    if data is None or not isinstance(data, dict):
        print(f"âŒ é˜Ÿåˆ—æ–‡ä»¶åŠ è½½å¤±è´¥: {queue_file}")
        print("ğŸ’¡ è¯·å…ˆè¿è¡Œ: python ai_poadcast_main/collect_rss_feeds.py")
        return {'items': []}

    items = data.get('items', [])
    print(f"[DEBUG] åŠ è½½æˆåŠŸï¼Œå…± {len(items)} æ¡")
    
    # è¿‡æ»¤å·²è·³è¿‡çš„URL
    skipped_urls = load_skipped_urls()
    if skipped_urls:
        before = len(items)
        items = [item for item in items if item.get('url') not in skipped_urls]
        filtered_count = before - len(items)
        if filtered_count > 0:
            print(f"[DEBUG] å·²è¿‡æ»¤ {filtered_count} æ¡ä¹‹å‰è·³è¿‡çš„æ–°é—»")
        data['items'] = items

    if summaries_file:
        from error_utils import safe_json_read
        summaries_path = safe_path(summaries_file, Path.cwd())
        print(f"[DEBUG] å°è¯•åˆå¹¶ä¸­æ–‡è¦ç‚¹: {summaries_file}")
        
        summaries_data = safe_json_read(summaries_path)
        if summaries_data:
            summary_lookup = {}
            for entry in summaries_data.get("items", []):
                key = entry.get("url") or entry.get("title")
                if key:
                    summary_lookup[key] = entry
            merged = 0
            for item in items:
                key = item.get("url") or item.get("title")
                summary_entry = summary_lookup.get(key)
                if summary_entry:
                    if summary_entry.get("chinese_summary"):
                        item["chinese_summary"] = summary_entry["chinese_summary"]
                    if summary_entry.get("article_length") is not None:
                        item["article_length"] = summary_entry["article_length"]
                    merged += 1
            print(f"[DEBUG] å·²åˆå¹¶ä¸­æ–‡è¦ç‚¹ {merged} æ¡")
        else:
            print("[DEBUG] ä¸­æ–‡è¦ç‚¹æ–‡ä»¶åŠ è½½å¤±è´¥æˆ–ä¸å­˜åœ¨ã€‚")
    
    return data

def filter_by_keywords(items, must_include=None, must_exclude=None):
    """æŒ‰å…³é”®è¯è¿‡æ»¤"""
    if must_include is None:
        must_include = [
            # æ ¸å¿ƒå…³é”®è¯ - é«˜æƒé‡
            'visa', 'immigration', 'policy', 'international student',
            'study abroad', 'admission', 'application', 'scholarship',
            'university ranking', 'college ranking', 'tuition',
            # è€ƒè¯•ç›¸å…³
            'ielts', 'toefl', 'gre', 'gmat', 'sat', 'act',
            # å­¦ä½ç›¸å…³
            'master', 'phd', 'graduate', 'undergraduate', 'mba',
            # åœ°åŒºç›¸å…³
            'uk visa', 'us visa', 'canada visa', 'australia visa',
            # æœºæ„ç›¸å…³
            'university', 'college', 'education', 'academic'
        ]
    
    if must_exclude is None:
        must_exclude = [
            # æ˜ç¡®æ’é™¤çš„å†…å®¹
            'sport', 'football', 'basketball', 'celebrity', 'entertainment',
            'gossip', 'fashion', 'beauty', 'recipe', 'cooking', 'game', 'gaming',
            # ä½è´¨é‡å†…å®¹
            'weather', 'traffic', 'local news', 'obituary', 'crime',
            # éæ•™è‚²ç›¸å…³
            'real estate', 'property', 'investment', 'stock market',
            'cryptocurrency', 'bitcoin', 'trading'
        ]
    
    print(f"[DEBUG] å¼€å§‹å…³é”®è¯è¿‡æ»¤ï¼Œè¾“å…¥ {len(items)} æ¡")

    source_exclude = {
        "UK GOV Education",
    }
    source_keyword_rules = {
        "UK GOV Education": [
            "international student",
            "overseas student",
            "international education",
            "visa",
            "immigration",
            "foreign student",
            "study abroad",
            "international recruitment",
        ],
    }
    
    filtered = []
    for item in items:
        title_lower = item['title'].lower()
        summary_lower = item.get('summary', '').lower()
        combined = title_lower + ' ' + summary_lower
        source_name = item.get('source', '')

        if source_name in source_exclude:
            allow_keywords = source_keyword_rules.get(source_name, [])
            allow = any(keyword in combined for keyword in allow_keywords)
            if not allow:
                print(f"[DEBUG] æ’é™¤: {item['title'][:50]} (æ¥æºè¿‡æ»¤: {source_name})")
                continue
        
        # æ£€æŸ¥æ’é™¤è¯
        excluded = False
        for word in must_exclude:
            if word.lower() in combined:
                excluded = True
                print(f"[DEBUG] æ’é™¤: {item['title'][:50]} (åŒ¹é…æ’é™¤è¯: {word})")
                break
        
        if excluded:
            continue
        
        # æ£€æŸ¥åŒ…å«è¯ï¼ˆæˆ–è€…ä¼˜å…ˆçº§>=9ç›´æ¥é€šè¿‡ï¼‰
        if item.get('priority', 0) >= 9:
            print(f"[DEBUG] ä¿ç•™ï¼ˆé«˜ä¼˜å…ˆçº§ï¼‰: {item['title'][:50]}")
            filtered.append(item)
        elif any(word.lower() in combined for word in must_include):
            print(f"[DEBUG] ä¿ç•™ï¼ˆå…³é”®è¯åŒ¹é…ï¼‰: {item['title'][:50]}")
            filtered.append(item)
        else:
            print(f"[DEBUG] æ’é™¤ï¼ˆæ— å…³é”®è¯ï¼‰: {item['title'][:50]}")
    
    print(f"[DEBUG] è¿‡æ»¤å®Œæˆï¼Œè¾“å‡º {len(filtered)} æ¡")
    return filtered

def import_story(item, dry_run=False):
    """è°ƒç”¨ import_raw_story.py å¯¼å…¥"""
    cmd = [
        'python', 'ai_poadcast_main/import_raw_story.py',
        '--title', item['title'],
        '--url', item['url'],
    ]

    source = item.get('source')
    if source:
        cmd.extend(['--source', source])

    published_date = item.get('published_date') or _extract_iso_date(item.get('published', ''))
    if published_date:
        cmd.extend(['--published-date', published_date])

    cmd.extend(['--fetch', '--store-html'])

    for tag in item.get('tags', []):
        cmd.extend(['--tags', tag])
    
    if dry_run:
        print("  [DRY RUN] " + ' '.join(cmd))
        return True
    
    try:
        result = subprocess.run(cmd, capture_output=True, text=True, check=True)
        print("  âœ… å¯¼å…¥æˆåŠŸ")
        return True
    except subprocess.CalledProcessError as e:
        if "URL already exists" in e.stderr or "è¯¥ URL å·²ç»å­˜æ¡£" in e.stderr:
            print("  âš ï¸  å·²å­˜åœ¨ï¼Œè·³è¿‡")
            return True  # è¿”å›Trueé¿å…ä¸­æ–­æµç¨‹
        else:
            print(f"  âŒ å¯¼å…¥å¤±è´¥: {e.stderr[:100]}")
            return False


def ensure_import_metadata(item: dict) -> bool:
    """ç¡®ä¿å¯¼å…¥å‰çš„å¿…è¦å­—æ®µé½å…¨ï¼Œå¿…è¦æ—¶è¯·æ±‚ç”¨æˆ·è¾“å…¥"""
    title = item.get('title', '').strip()
    url = item.get('url', '').strip()
    if not title:
        print("  âŒ ç¼ºå°‘æ ‡é¢˜ï¼Œæ— æ³•å¯¼å…¥ã€‚")
        return False
    if not url:
        print("  âŒ ç¼ºå°‘ URLï¼Œæ— æ³•å¯¼å…¥ã€‚")
        return False

    source = (item.get('source') or "").strip()
    if not source:
        user_source = input("æ¥æºç¼ºå¤±ï¼Œè¾“å…¥æ¥æºåç§°ï¼ˆå›è½¦ä½¿ç”¨ Unknownï¼‰: ").strip()
        item['source'] = user_source or "Unknown"
    else:
        item['source'] = source

    if not item.get('published_date'):
        inferred = _extract_iso_date(item.get('published', ''))
        if inferred:
            item['published_date'] = inferred
            print(f"  ğŸ—“ï¸  ä½¿ç”¨åŸå§‹å‘å¸ƒæ—¥æœŸ: {inferred}")
        else:
            default_date = date.today().isoformat()
            while True:
                user_input = input(f"å‘å¸ƒæ—¥æœŸæœªçŸ¥ï¼Œè¯·è¾“å…¥æ—¥æœŸ (YYYY-MM-DDï¼Œå›è½¦ä½¿ç”¨ {default_date}): ").strip()
                candidate = user_input or default_date
                normalized = _validate_iso_date(candidate)
                if normalized:
                    item['published_date'] = normalized
                    break
                print("  âš ï¸ æ—¥æœŸæ ¼å¼ä¸æ­£ç¡®ï¼Œè¯·é‡æ–°è¾“å…¥ã€‚")

    tags = item.get('tags')
    if not tags:
        tag_input = input("æœªè®¾ç½®æ ‡ç­¾ï¼Œå¯è¾“å…¥é€—å·åˆ†éš”çš„æ ‡ç­¾ï¼ˆå›è½¦è·³è¿‡ï¼‰: ").strip()
        if tag_input:
            item['tags'] = [tag.strip() for tag in tag_input.split(',') if tag.strip()]

    return True

def interactive_review(items, max_import=10, translator=None):
    """äº¤äº’å¼å®¡æ ¸"""
    print("\n" + "="*70)
    print("ğŸ“‹ å¾…å¤„ç†æ–°é—»åˆ—è¡¨ï¼ˆæŒ‰ä¼˜å…ˆçº§æ’åºï¼‰")
    print("="*70)
    
    imported_count = 0
    skipped_count = 0
    
    for i, item in enumerate(items, 1):
        if imported_count >= max_import:
            print(f"\nâš ï¸  å·²è¾¾åˆ°å¯¼å…¥ä¸Šé™ ({max_import} æ¡)")
            break
        
        print("\n" + "="*70)
        print(f"[{i}/{len(items)}] ä¼˜å…ˆçº§: {item['priority']}")
        print(f"æ¥æº: {item['source']}")
        print(f"æ ‡é¢˜: {item['title']}")
        print(f"URL: {item['url']}")
        
        published_date = item.get('published_date') or _extract_iso_date(item.get('published', ''))
        if published_date:
            print(f"å‘å¸ƒæ—¥æœŸ: {published_date}")
        else:
            print(f"å‘å¸ƒæ—¥æœŸ: æœªçŸ¥")

        cn_summary = item.get('chinese_summary')
        if cn_summary:
            print("\nã€ä¸­æ–‡è¦ç‚¹ã€‘")
            print(cn_summary.strip())
        
        if item.get('summary'):
            summary_text = item['summary']
            display_text = summary_text[:150]
            print(f"æ‘˜è¦ (åŸæ–‡): {display_text}...")
            zh_translation = translate_summary_to_zh(summary_text, translator)
            if zh_translation:
                print(f"æ‘˜è¦ (ä¸­æ–‡): {zh_translation}")
        
        print("="*70)
        
        choice = input("\næ“ä½œ: [y]å¯¼å…¥ [n]è·³è¿‡ [s]åœæ­¢ [o]æ‰“å¼€æµè§ˆå™¨ [q]é€€å‡º? ").lower()
        
        if choice == 'y':
            if ensure_import_metadata(item):
                if import_story(item):
                    imported_count += 1
                else:
                    skipped_count += 1
            else:
                skipped_count += 1
        elif choice == 'n':
            skipped_count += 1
            save_skipped_url(item['url'])
            print("  â­ï¸  å·²è·³è¿‡ï¼ˆå·²è®°å½•ï¼Œä¸‹æ¬¡ä¸å†æ˜¾ç¤ºï¼‰")
        elif choice == 's':
            print("\nğŸ›‘ åœæ­¢å®¡æ ¸")
            break
        elif choice == 'o':
            import webbrowser
            webbrowser.open(item['url'])
            choice2 = input("çœ‹å®Œäº†å—ï¼Ÿ[y]å¯¼å…¥ [n]è·³è¿‡? ").lower()
            if choice2 == 'y':
                if import_story(item):
                    imported_count += 1
                else:
                    skipped_count += 1
            else:
                skipped_count += 1
                save_skipped_url(item['url'])
        elif choice == 'q':
            print("\nğŸ‘‹ é€€å‡º")
            return
    
    print("\n" + "="*70)
    print("âœ… å®¡æ ¸å®Œæˆ")
    print(f"  å¯¼å…¥: {imported_count} æ¡")
    print(f"  è·³è¿‡: {skipped_count} æ¡")
    print("="*70)

def auto_import_top(items, count=5):
    """è‡ªåŠ¨å¯¼å…¥ä¼˜å…ˆçº§æœ€é«˜çš„Næ¡"""
    print(f"\nğŸ¤– è‡ªåŠ¨å¯¼å…¥æ¨¡å¼ï¼šå°†å¯¼å…¥å‰ {count} æ¡é«˜ä¼˜å…ˆçº§æ–°é—»\n")
    
    imported = 0
    for i, item in enumerate(items[:count], 1):
        title_short = item['title'][:60]
        print(f"[{i}/{count}] {title_short}...")
        if import_story(item):
            imported += 1
    
    print(f"\nâœ… æˆåŠŸå¯¼å…¥ {imported}/{count} æ¡")

def show_summary(items):
    """æ˜¾ç¤ºé˜Ÿåˆ—æ‘˜è¦"""
    print("\n" + "="*70)
    print("ğŸ“Š é˜Ÿåˆ—æ‘˜è¦")
    print("="*70)
    
    by_source = {}
    for item in items:
        source = item['source']
        by_source[source] = by_source.get(source, 0) + 1
    
    print("\næŒ‰æ¥æºåˆ†å¸ƒï¼š")
    for source, count in sorted(by_source.items(), key=lambda x: x[1], reverse=True):
        print(f"  {source}: {count} æ¡")
    
    by_priority = {}
    for item in items:
        priority = item['priority']
        by_priority[priority] = by_priority.get(priority, 0) + 1
    
    print("\næŒ‰ä¼˜å…ˆçº§åˆ†å¸ƒï¼š")
    for priority in sorted(by_priority.keys(), reverse=True):
        print(f"  ä¼˜å…ˆçº§ {priority}: {by_priority[priority]} æ¡")

def main():
    """ä¸»æµç¨‹"""
    import argparse
    
    parser = argparse.ArgumentParser(description='å¤„ç†æ–°é—»é˜Ÿåˆ—')
    parser.add_argument('--auto', type=int, help='è‡ªåŠ¨å¯¼å…¥å‰Næ¡')
    parser.add_argument('--queue-file', default='ai_poadcast_main/news_queue.json',
                        help='é˜Ÿåˆ—æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--summaries-file', default='ai_poadcast_main/news_queue_with_summaries.json',
                        help='åŒ…å«ä¸­æ–‡è¦ç‚¹çš„æ–‡ä»¶ï¼Œå­˜åœ¨æ—¶è‡ªåŠ¨åˆå¹¶')
    parser.add_argument('--no-summaries', action='store_true', help='å¿½ç•¥ä¸­æ–‡è¦ç‚¹åˆå¹¶')
    parser.add_argument('--min-priority', type=int, default=7, help='æœ€ä½ä¼˜å…ˆçº§')
    parser.add_argument('--max-import', type=int, default=10, help='æœ€å¤šå¯¼å…¥æ•°é‡')
    parser.add_argument('--summary', action='store_true', help='åªæ˜¾ç¤ºæ‘˜è¦ï¼Œä¸å¤„ç†')
    parser.add_argument('--no-filter', action='store_true', help='ä¸è¿‡æ»¤å…³é”®è¯')
    parser.add_argument('--debug', action='store_true', help='æ˜¾ç¤ºè°ƒè¯•ä¿¡æ¯')
    parser.add_argument('--translate', action='store_true', help='å°†æ–°é—»æ‘˜è¦ç¿»è¯‘æˆä¸­æ–‡ï¼ˆéœ€é…ç½®å¯¹åº” API KEYï¼‰')
    parser.add_argument('--translate-provider', choices=['openai', 'anthropic'], default='openai',
                        help='æ‘˜è¦ç¿»è¯‘æ¨¡å‹æä¾›æ–¹ï¼ˆé»˜è®¤ï¼šopenaiï¼‰')
    parser.add_argument('--translate-model', default='gpt-4o-mini', help='æ‘˜è¦ç¿»è¯‘ä½¿ç”¨çš„æ¨¡å‹åç§°')
    args = parser.parse_args()
    
    print("[DEBUG] å¼€å§‹æ‰§è¡Œ")
    
    translator = None
    if args.translate:
        translator = init_translator(args.translate_provider, args.translate_model)
        if not translator:
            print("âš ï¸ æœªèƒ½å¯ç”¨æ‘˜è¦ç¿»è¯‘åŠŸèƒ½ï¼Œå°†ç»§ç»­æ˜¾ç¤ºè‹±æ–‡æ‘˜è¦ã€‚")
    
    # åŠ è½½é˜Ÿåˆ—
    summaries_file = None if args.no_summaries else args.summaries_file
    queue = load_queue(args.queue_file, summaries_file)
    items = queue.get('items', [])
    
    print(f"[DEBUG] itemsæ•°é‡: {len(items)}")
    
    if not items:
        print("\nâš ï¸  é˜Ÿåˆ—ä¸ºç©º")
        print("ğŸ’¡ è¯·è¿è¡Œ: python ai_poadcast_main/collect_rss_feeds.py")
        return
    
    print(f"ğŸ“¥ é˜Ÿåˆ—ä¸­å…±æœ‰ {len(items)} æ¡æ–°é—»")
    
    # ä¼˜å…ˆçº§è¿‡æ»¤
    original_count = len(items)
    items = [item for item in items if item.get('priority', 0) >= args.min_priority]
    print(f"ğŸ¯ è¿‡æ»¤åå‰©ä½™ {len(items)} æ¡ï¼ˆä¼˜å…ˆçº§ >= {args.min_priority}ï¼Œè¿‡æ»¤æ‰ {original_count - len(items)} æ¡ï¼‰")
    
    # å…³é”®è¯è¿‡æ»¤
    if not args.no_filter:
        before_filter = len(items)
        items = filter_by_keywords(items)
        print(f"ğŸ” å…³é”®è¯è¿‡æ»¤åå‰©ä½™ {len(items)} æ¡ï¼ˆè¿‡æ»¤æ‰ {before_filter - len(items)} æ¡ï¼‰")
    
    if not items:
        print("\nâš ï¸  æ²¡æœ‰ç¬¦åˆæ¡ä»¶çš„æ–°é—»")
        print("ğŸ’¡ å°è¯•é™ä½ä¼˜å…ˆçº§: --min-priority 6")
        print("ğŸ’¡ æˆ–è·³è¿‡å…³é”®è¯è¿‡æ»¤: --no-filter")
        return
    
    # æŒ‰ä¼˜å…ˆçº§æ’åº
    items.sort(key=lambda x: x.get('priority', 0), reverse=True)
    
    # æ˜¾ç¤ºæ‘˜è¦
    if args.summary:
        show_summary(items)
        return
    
    # å¤„ç†
    if args.auto:
        auto_import_top(items, args.auto)
    else:
        interactive_review(items, args.max_import, translator)

if __name__ == "__main__":
    main()
