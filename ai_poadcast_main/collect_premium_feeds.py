#!/usr/bin/env python3
"""
é«˜è´¨é‡æ–°é—»æºé‡‡é›†å™¨ - ä¸“æ³¨æƒå¨æº
"""

import json
import time
from datetime import datetime, timezone
from pathlib import Path
from premium_feeds_config import PREMIUM_SOURCES, QUALITY_FILTERS
from collect_rss_feeds import (
    parse_feed_with_fallback, normalize_url, 
    deduplicate_by_title, load_seen_urls
)

def collect_premium_news():
    """é‡‡é›†é«˜è´¨é‡æ–°é—»æº"""
    print("ğŸ”¥ å¯åŠ¨é«˜è´¨é‡æ–°é—»é‡‡é›†...")
    
    seen_urls = load_seen_urls()
    all_items = []
    
    for source_name, config in PREMIUM_SOURCES.items():
        if config.get('method') == 'scrape':
            print(f"âš ï¸ {source_name}: éœ€è¦çˆ¬è™«ï¼Œè·³è¿‡")
            continue
            
        rss_url = config.get('rss')
        if not rss_url:
            continue
            
        print(f"ğŸ“¡ é‡‡é›†: {source_name}")
        
        feed = parse_feed_with_fallback(source_name, rss_url)
        if not feed:
            continue
            
        max_items = config.get('max_items', 3)
        priority = config.get('priority', 7)
        
        count = 0
        for entry in feed.entries:
            if count >= max_items:
                break
                
            url = normalize_url(getattr(entry, 'link', ''))
            if not url or url in seen_urls:
                continue
                
            # è´¨é‡è¿‡æ»¤
            title = entry.title
            summary = getattr(entry, 'summary', '')
            
            # æ£€æŸ¥æœ€å°‘å­—æ•°
            if len(title + summary) < QUALITY_FILTERS['min_word_count']:
                continue
                
            # æ£€æŸ¥æ’é™¤è¯
            text_lower = (title + ' ' + summary).lower()
            if any(word in text_lower for word in QUALITY_FILTERS['exclude_keywords']):
                continue
                
            all_items.append({
                'title': title,
                'url': url,
                'source': source_name,
                'published': getattr(entry, 'published', ''),
                'summary': summary[:200],
                'tags': config.get('tags', []),
                'priority': priority,
                'collected_at': datetime.now(timezone.utc).isoformat()
            })
            count += 1
            
        print(f"  âœ… é‡‡é›† {count} æ¡")
        time.sleep(1)  # é¿å…è¯·æ±‚è¿‡å¿«
    
    # å»é‡
    unique_items = deduplicate_by_title(all_items)
    
    # ä¿å­˜
    if unique_items:
        output_file = Path("ai_poadcast_main/premium_news_queue.json")
        output_file.parent.mkdir(exist_ok=True)
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump({
                'updated_at': datetime.now(timezone.utc).isoformat(),
                'total': len(unique_items),
                'items': sorted(unique_items, key=lambda x: x['priority'], reverse=True)
            }, f, ensure_ascii=False, indent=2)
            
        print(f"ğŸ’¾ ä¿å­˜ {len(unique_items)} æ¡é«˜è´¨é‡æ–°é—»")
        return unique_items
    
    return []

if __name__ == "__main__":
    collect_premium_news()