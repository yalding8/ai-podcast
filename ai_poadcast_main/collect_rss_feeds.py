#!/usr/bin/env python3
"""
RSS æ‰¹é‡é‡‡é›†å™¨
æ¯å¤©è‡ªåŠ¨æ‹‰å–æ‰€æœ‰RSSæºï¼Œå»é‡åä¿å­˜åˆ°å¾…å¤„ç†é˜Ÿåˆ—
"""

import json
import re
import time
from datetime import datetime, timedelta, timezone
from pathlib import Path
from typing import Optional
from urllib.parse import urlparse

import feedparser
import requests

# å¯¼å…¥é…ç½®
try:
    from feeds_config import TIER_1_SOURCES, TIER_2_SOURCES, TIER_3_SOURCES
except ImportError:
    import sys
    from pathlib import Path
    sys.path.insert(0, str(Path(__file__).parent.parent))
    try:
        from config import RSS_SOURCES
        TIER_1_SOURCES = {k: v for k, v in RSS_SOURCES.items() if v.get('priority', 0) >= 9}
        TIER_2_SOURCES = {k: v for k, v in RSS_SOURCES.items() if 7 <= v.get('priority', 0) < 9}
        TIER_3_SOURCES = {k: v for k, v in RSS_SOURCES.items() if v.get('priority', 0) < 7}
    except ImportError:
        TIER_1_SOURCES = {}
        TIER_2_SOURCES = {}
        TIER_3_SOURCES = {}


BASE_DIR = Path(__file__).resolve().parent
FAIL_LOG_DIR = BASE_DIR / "logs"
FAIL_LOG_PATH = FAIL_LOG_DIR / "rss_failures.log"
FAILURE_RECORDS = []
RUN_SUMMARY: dict[str, dict] = {}

def normalize_url(url):
    """æ ‡å‡†åŒ–URLï¼Œå»é™¤æŸ¥è¯¢å‚æ•°"""
    parsed = urlparse(url)
    return f"{parsed.scheme}://{parsed.netloc}{parsed.path}"

# åœ¨ collect_rss_feeds.py ä¸­æ·»åŠ 
from difflib import SequenceMatcher

def similar(a, b):
    """è®¡ç®—æ ‡é¢˜ç›¸ä¼¼åº¦"""
    return SequenceMatcher(None, a.lower(), b.lower()).ratio()

def deduplicate_by_title(items, threshold=0.85):
    """æŒ‰æ ‡é¢˜å»é‡"""
    unique = []
    seen_titles = []
    
    for item in items:
        is_duplicate = False
        for seen in seen_titles:
            if similar(item['title'], seen) > threshold:
                is_duplicate = True
                break
        
        if not is_duplicate:
            unique.append(item)
            seen_titles.append(item['title'])
    
    return unique

def generate_slug(title):
    """ç”ŸæˆURLå‹å¥½çš„slug"""
    import re
    slug = re.sub(r'[^\w\s-]', '', title.lower())
    slug = re.sub(r'[-\s]+', '-', slug)
    return slug[:50]

def load_seen_urls():
    """åŠ è½½å·²é‡‡é›†çš„URLåˆ—è¡¨"""
    from path_utils import safe_path
    from error_utils import safe_json_read
    
    index_file = safe_path("source_archive/_index.json", Path.cwd())
    data = safe_json_read(index_file, default=[])
    
    if isinstance(data, dict):
        sources = data.get('sources', [])
    elif isinstance(data, list):
        sources = data
    else:
        sources = []
    
    return {item['url'] for item in sources if isinstance(item, dict) and item.get('url')}

def _record_failure(source: str, rss_url: str, stage: str, error: str) -> None:
    timestamp = datetime.now(timezone.utc).isoformat(timespec="seconds")
    FAILURE_RECORDS.append({
        "timestamp": timestamp,
        "source": source,
        "rss": rss_url,
        "stage": stage,
        "error": error,
    })
    summary = RUN_SUMMARY.setdefault(source, {
        "status": "error",
        "rss": rss_url,
        "raw_new_items": 0,
        "final_items": 0,
        "priority": None,
    })
    summary.update({
        "status": "error",
        "reason": error,
    })


def _request_feed(source: str, rss_url: str, retries: int = 3, backoff: float = 3.0) -> Optional[requests.Response]:
    from error_utils import safe_http_get
    
    headers = {
        "User-Agent": "Mozilla/5.0 (RSS Collector)",
        "Accept": "application/rss+xml, application/atom+xml, application/xml;q=0.9, text/xml;q=0.8, */*;q=0.1",
    }
    
    response = safe_http_get(rss_url, timeout=20, max_retries=retries, headers=headers)
    if response is None:
        error_text = f"RSSè¯·æ±‚å¤±è´¥ï¼Œå·²é‡è¯•{retries}æ¬¡"
        print(f"  âŒ {error_text}")
        _record_failure(source, rss_url, "request", error_text)
    
    return response


def _clean_html(text: str) -> str:
    """ç§»é™¤å¸¸è§çš„éæ³• XML å­—ç¬¦å¹¶ä¿®å¤æœªé—­åˆå®ä½“ã€‚"""
    cleaned = text.replace("&nbsp;", " ")
    cleaned = re.sub(r"[\x00-\x08\x0B\x0C\x0E-\x1F]", "", cleaned)
    return cleaned


def parse_feed_with_fallback(source: str, rss_url: str) -> Optional[feedparser.FeedParserDict]:
    response = _request_feed(source, rss_url)
    if not response:
        return None

    candidates = [response.content]
    # å°è¯•åœ¨å¿½ç•¥éæ³•å­—ç¬¦åçš„æ–‡æœ¬ä¸Šé‡æ–°è§£æ
    cleaned_text = _clean_html(response.text)
    candidates.append(cleaned_text.encode(response.encoding or "utf-8", errors="ignore"))

    last_error = None
    for data in candidates:
        parsed = feedparser.parse(data)
        if not parsed.bozo:
            return parsed
        last_error = parsed.bozo_exception

    snippet = cleaned_text[:200].replace("\n", " ")
    error_text = f"{last_error} | ç‰‡æ®µ: {snippet}..."
    print(f"  âŒ RSSè§£æå¤±è´¥: {last_error} | ç‰‡æ®µ: {snippet}...")
    _record_failure(source, rss_url, "parse", error_text)
    return None


def fetch_rss(source_name, config, seen_urls, min_age_hours=0):
    """
    æ‹‰å–å•ä¸ªRSSæº
    
    Args:
        min_age_hours: åªé‡‡é›†Nå°æ—¶å†…çš„æ–°é—»ï¼ˆ0=ä¸é™åˆ¶ï¼‰
    """
    print(f"\nğŸ“¡ æ­£åœ¨æ‹‰å–: {source_name}")

    summary = RUN_SUMMARY.setdefault(source_name, {
        "status": "pending",
        "rss": config.get('rss', ''),
        "raw_new_items": 0,
        "final_items": 0,
        "priority": config.get('priority', 5),
    })

    if config.get('method') == 'scrape':
        print("  âš ï¸  éœ€è¦çˆ¬è™«ï¼Œè·³è¿‡ï¼ˆç¨åæ‰‹åŠ¨å¤„ç†ï¼‰")
        summary.update({
            "status": "skipped",
            "reason": "requires scraper",
        })
        return []

    rss_url = config.get('rss')
    if not rss_url:
        print("  âŒ æœªé…ç½®RSS")
        summary.update({
            "status": "error",
            "reason": "rss url missing",
        })
        return []

    feed = parse_feed_with_fallback(source_name, rss_url)
    if feed is None:
        summary.update({
            "status": "error",
            "reason": "fetch/parse failed",
        })
        return []
    
    new_items = []
    cutoff_time = datetime.now(timezone.utc) - timedelta(hours=min_age_hours) if min_age_hours > 0 else None
    
    if config.get('max_items') is not None:
        max_items = int(config['max_items'])
    else:
        max_items = 10 if 'Google News' in source_name else 20
    
    collected = 0
    for entry in feed.entries:
        if collected >= max_items:
            break

        # æ ‡å‡†åŒ–URL
        link = getattr(entry, 'link', None)
        if not link:
            continue
        url = normalize_url(link)
        
        # å»é‡æ£€æŸ¥
        if url in seen_urls:
            continue
        
        # æ£€æŸ¥å‘å¸ƒæ—¶é—´
        if cutoff_time and hasattr(entry, 'published_parsed'):
            pub_time = datetime(*entry.published_parsed[:6])
            if pub_time < cutoff_time:
                continue
        
        new_items.append({
            'title': entry.title,
            'url': url,
            'source': source_name,
            'published': entry.get('published', ''),
            'summary': entry.get('summary', '')[:200],
            'tags': config.get('tags', []),
            'priority': config.get('priority', 5),
            'collected_at': datetime.now(timezone.utc).isoformat()
        })
        collected += 1
    
    print(f"  âœ… å‘ç° {len(new_items)} æ¡æ–°å†…å®¹")
    summary.update({
        "status": "success",
        "raw_new_items": len(new_items),
    })
    return new_items

def save_queue(items, output_file="ai_poadcast_main/news_queue.json"):
    """ä¿å­˜åˆ°å¾…å¤„ç†é˜Ÿåˆ—"""
    from path_utils import safe_path
    from error_utils import safe_json_write
    
    output_path = safe_path(output_file, Path.cwd())
    
    # æŒ‰ä¼˜å…ˆçº§æ’åº
    items.sort(key=lambda x: x['priority'], reverse=True)
    
    data = {
        'updated_at': datetime.now(timezone.utc).isoformat(),
        'total': len(items),
        'items': items
    }
    
    if safe_json_write(output_path, data):
        print(f"\nğŸ’¾ å·²ä¿å­˜ {len(items)} æ¡æ–°é—»åˆ°é˜Ÿåˆ—: {output_path}")
    else:
        print(f"\nâŒ ä¿å­˜é˜Ÿåˆ—å¤±è´¥: {output_path}")

def main():
    """ä¸»æµç¨‹"""
    print("ğŸš€ å¼€å§‹é‡‡é›†RSSæ–°é—»æº...\n")
    
    # åŠ è½½å·²è§URL
    seen_urls = load_seen_urls()
    print(f"ğŸ“š å·²æœ‰ {len(seen_urls)} æ¡å†å²è®°å½•")
    
    # åˆå¹¶æ‰€æœ‰æº
    all_sources = {**TIER_1_SOURCES, **TIER_2_SOURCES, **TIER_3_SOURCES}
    
    # é‡‡é›†
    all_items = []
    for name, config in all_sources.items():
        items = fetch_rss(name, config, seen_urls, min_age_hours=0)  # ä¸é™åˆ¶æ—¶é—´
        all_items.extend(items)
    
    if all_items:
        before = len(all_items)
        all_items = deduplicate_by_title(all_items)
        if len(all_items) != before:
            print(f"\nğŸ§¹ æ ‡é¢˜å»é‡ï¼šä» {before} æ¡ç¼©å‡åˆ° {len(all_items)} æ¡")

    final_counts = {}
    for item in all_items:
        source = item['source']
        final_counts[source] = final_counts.get(source, 0) + 1

    for source, data in RUN_SUMMARY.items():
        data['final_items'] = final_counts.get(source, 0)
        if data.get('status') == 'pending':
            data['status'] = 'success'
    for source, count in final_counts.items():
        if source not in RUN_SUMMARY:
            RUN_SUMMARY[source] = {
                "status": "success",
                "rss": all_sources.get(source, {}).get('rss', ''),
                "raw_new_items": count,
                "final_items": count,
                "priority": all_sources.get(source, {}).get('priority', 5),
            }
    
    # ä¿å­˜
    if all_items:
        save_queue(all_items)
        
        # ç”Ÿæˆå¯è¯»æŠ¥å‘Š
        print("\n" + "="*60)
        print("ğŸ“Š é‡‡é›†æ±‡æ€»:")
        print("="*60)
        
        by_source = {}
        for item in all_items:
            source = item['source']
            by_source[source] = by_source.get(source, 0) + 1
        
        for source, count in sorted(by_source.items(), key=lambda x: x[1], reverse=True):
            print(f"  {source}: {count} æ¡")
        
        print(f"\nâœ… æ€»è®¡: {len(all_items)} æ¡æ–°å†…å®¹")
    else:
        print("\nâš ï¸  æ²¡æœ‰å‘ç°æ–°å†…å®¹")

    summary_payload = {
        "run_at": datetime.now(timezone.utc).isoformat(timespec='seconds'),
        "total_items": len(all_items),
        "sources": RUN_SUMMARY,
    }
    FAIL_LOG_DIR.mkdir(parents=True, exist_ok=True)
    summary_path = FAIL_LOG_DIR / "rss_run_summary.json"
    summary_path.write_text(json.dumps(summary_payload, ensure_ascii=False, indent=2), encoding='utf-8')
    print(f"\nğŸ“ RSS æ±‡æ€»å†™å…¥: {summary_path}")

    if FAILURE_RECORDS:
        with FAIL_LOG_PATH.open('a', encoding='utf-8') as logf:
            logf.write("\n" + "=" * 80 + "\n")
            logf.write(f"Run at {datetime.now(timezone.utc).isoformat(timespec='seconds')}\n")
            for record in FAILURE_RECORDS:
                logf.write(
                    f"[{record['timestamp']}] stage={record['stage']} | source={record['source']}\n"
                    f"  rss: {record['rss']}\n"
                    f"  error: {record['error']}\n"
                )
        print(f"\nâš ï¸  {len(FAILURE_RECORDS)} ä¸ª RSS æºå¤±è´¥ï¼Œè¯¦æƒ…è§ {FAIL_LOG_PATH}")

if __name__ == "__main__":
    main()
