#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""Generate podcast script via LLM using a Stage 3 prompt file."""

from __future__ import annotations

import argparse
import json
import os
import sys
from datetime import date
from pathlib import Path
from typing import Optional

try:
    from openai import OpenAI  # type: ignore
except ImportError:  # pragma: no cover
    OpenAI = None

try:
    import anthropic  # type: ignore
except ImportError:  # pragma: no cover
    anthropic = None

try:
    import requests
except ImportError:  # pragma: no cover
    requests = None

DEFAULT_PROVIDER = os.getenv("STAGE3_PROVIDER") or os.getenv("KEYPOINT_PROVIDER") or "deepseek"
DEFAULT_MODEL = os.getenv("STAGE3_MODEL") or os.getenv("KEYPOINT_MODEL") or "deepseek-chat"
DEFAULT_SUMMARY_PATH = Path("ai_poadcast_main/news_queue_with_summaries.json")
STAGE3_INPUT_DIR = Path("ai_poadcast_main/stage3_inputs")
STAGE3_OUTPUT_DIR = Path("è„šæœ¬è¾“å‡º")


def parse_args() -> argparse.Namespace:
    parser = argparse.ArgumentParser(description="Generate Stage 3 podcast script from prompt file.")
    parser.add_argument("--prompt", help="Path to Stage 3 prompt file (Markdown).")
    parser.add_argument("--date", help="Episode date (YYYY-MM-DD); derives prompt/output paths when --prompt æœªæŒ‡å®šã€‚")
    parser.add_argument("--provider", default=DEFAULT_PROVIDER, help="LLM provider (openai/anthropic/deepseek)ã€‚")
    parser.add_argument("--model", default=DEFAULT_MODEL, help="LLM model nameã€‚")
    parser.add_argument("--max-tokens", type=int, default=int(os.getenv("STAGE3_MAX_TOKENS", "1800")), help="Maximum tokens for response.")
    parser.add_argument("--temperature", type=float, default=float(os.getenv("STAGE3_TEMPERATURE", "0.4")), help="Sampling temperatureã€‚")
    parser.add_argument("--output", help="è‡ªå®šä¹‰è¾“å‡ºæ–‡ä»¶è·¯å¾„ï¼›é»˜è®¤å†™å…¥ è„šæœ¬è¾“å‡º/<date>/episode_<date>_v1.mdã€‚")
    parser.add_argument("--overwrite", action="store_true", help="å·²å­˜åœ¨ç›®æ ‡æ–‡ä»¶æ—¶è¦†ç›–ã€‚")
    parser.add_argument("--print-only", action="store_true", help="ä»…æ‰“å°ç”Ÿæˆç»“æœï¼Œä¸å†™å…¥æ–‡ä»¶ã€‚")
    return parser.parse_args()


def resolve_prompt_path(args: argparse.Namespace) -> tuple[Path, str]:
    from path_utils import safe_path
    if args.prompt:
        path = safe_path(args.prompt, Path.cwd())
        if not path.exists():
            sys.stderr.write(f"âŒ Prompt æ–‡ä»¶ä¸å­˜åœ¨ï¼š{path}\n")
            sys.exit(1)
        episode_date = args.date or path.stem.replace("episode_", "")[:10]
        return path, episode_date
    if not args.date:
        episode_date = date.today().isoformat()
    else:
        episode_date = args.date
    prompt_path = STAGE3_INPUT_DIR / episode_date / f"episode_{episode_date}_prompt.md"
    if not prompt_path.exists():
        sys.stderr.write(f"âŒ æœªæ‰¾åˆ°é»˜è®¤ Promptï¼š{prompt_path}\n")
        sys.exit(1)
    return prompt_path, episode_date


def read_prompt(path: Path) -> str:
    content = path.read_text(encoding="utf-8")
    if not content.strip():
        sys.stderr.write("âš ï¸ Prompt å†…å®¹ä¸ºç©ºã€‚\n")
    return content


def call_openai(model: str, prompt: str, temperature: float, max_tokens: int) -> str:
    if OpenAI is None:
        raise RuntimeError("æœªå®‰è£… openai åº“ã€‚")
    client = OpenAI()
    response = client.chat.completions.create(
        model=model,
        messages=[
            {"role": "system", "content": "You are a professional podcast script writer."},
            {"role": "user", "content": prompt},
        ],
        temperature=temperature,
        max_tokens=max_tokens,
    )
    return response.choices[0].message.content.strip()


def call_anthropic(model: str, prompt: str, temperature: float, max_tokens: int) -> str:
    if anthropic is None:
        raise RuntimeError("æœªå®‰è£… anthropic åº“ã€‚")
    client = anthropic.Anthropic()
    response = client.messages.create(
        model=model,
        system="You are a professional podcast script writer.",
        messages=[{"role": "user", "content": prompt}],
        max_tokens=max_tokens,
        temperature=temperature,
    )
    parts = []
    for block in response.content:
        text = getattr(block, "text", None)
        if text:
            parts.append(text.strip())
    return "\n".join(parts).strip()


def call_deepseek(model: str, prompt: str, temperature: float, max_tokens: int) -> str:
    if requests is None:
        raise RuntimeError("æœªå®‰è£… requests åº“ã€‚")
    api_key = os.getenv("DEEPSEEK_API_KEY") or os.getenv("DEEPSEEK_KEY")
    if not api_key:
        raise RuntimeError("ç¼ºå°‘ DEEPSEEK_API_KEYã€‚")
    base_url = os.getenv("DEEPSEEK_API_BASE", "https://api.deepseek.com").rstrip("/")
    endpoint = f"{base_url}/v1/chat/completions"
    payload = {
        "model": model,
        "messages": [
            {"role": "system", "content": "You are a professional podcast script writer."},
            {"role": "user", "content": prompt},
        ],
        "temperature": temperature,
        "max_tokens": max_tokens,
    }
    headers = {
        "Authorization": f"Bearer {api_key}",
        "Content-Type": "application/json",
    }
    resp = requests.post(endpoint, headers=headers, json=payload, timeout=60)
    if resp.status_code >= 400:
        try:
            detail = resp.json()
        except json.JSONDecodeError:
            detail = resp.text[:200]
        raise RuntimeError(f"DeepSeek API é”™è¯¯ï¼šHTTP {resp.status_code} {detail}")
    data = resp.json()
    choices = data.get("choices") or []
    if not choices:
        raise RuntimeError("DeepSeek è¿”å›ç¼ºå°‘ choicesã€‚")
    message = choices[0].get("message") or {}
    content = message.get("content", "").strip()
    if not content:
        raise RuntimeError("DeepSeek è¿”å›å†…å®¹ä¸ºç©ºã€‚")
    return content


def generate_script(provider: str, model: str, prompt: str, temperature: float, max_tokens: int) -> str:
    provider = provider.lower()
    
    # å®šä¹‰å¤‡ç”¨æ–¹æ¡ˆ
    fallback_providers = []
    if provider == "deepseek":
        fallback_providers = [("openai", "gpt-4o-mini"), ("anthropic", "claude-3-5-sonnet-20241022")]
    elif provider == "openai":
        fallback_providers = [("deepseek", "deepseek-chat"), ("anthropic", "claude-3-5-sonnet-20241022")]
    elif provider == "anthropic":
        fallback_providers = [("openai", "gpt-4o-mini"), ("deepseek", "deepseek-chat")]
    
    # å°è¯•ä¸»è¦provider
    try:
        if provider == "openai":
            return call_openai(model, prompt, temperature, max_tokens)
        if provider == "anthropic":
            return call_anthropic(model, prompt, temperature, max_tokens)
        if provider == "deepseek":
            return call_deepseek(model, prompt, temperature, max_tokens)
        raise RuntimeError(f"ä¸æ”¯æŒçš„ providerï¼š{provider}")
    except Exception as e:
        error_msg = str(e)
        # æ£€æŸ¥æ˜¯å¦æ˜¯503æˆ–æœåŠ¡è¿‡è½½é”™è¯¯
        if "503" in error_msg or "too busy" in error_msg.lower() or "service_unavailable" in error_msg.lower():
            sys.stderr.write(f"âš ï¸ {provider} æœåŠ¡ç¹å¿™ï¼Œå°è¯•åˆ‡æ¢å¤‡ç”¨LLM...\n")
            
            # å°è¯•å¤‡ç”¨providers
            for fallback_provider, fallback_model in fallback_providers:
                try:
                    sys.stderr.write(f"ğŸ”„ å°è¯•ä½¿ç”¨ {fallback_provider} ({fallback_model})...\n")
                    if fallback_provider == "openai":
                        result = call_openai(fallback_model, prompt, temperature, max_tokens)
                    elif fallback_provider == "anthropic":
                        result = call_anthropic(fallback_model, prompt, temperature, max_tokens)
                    elif fallback_provider == "deepseek":
                        result = call_deepseek(fallback_model, prompt, temperature, max_tokens)
                    else:
                        continue
                    sys.stderr.write(f"âœ… æˆåŠŸä½¿ç”¨ {fallback_provider}\n")
                    return result
                except Exception as fallback_error:
                    sys.stderr.write(f"âŒ {fallback_provider} ä¹Ÿå¤±è´¥: {fallback_error}\n")
                    continue
            
            # æ‰€æœ‰å¤‡ç”¨æ–¹æ¡ˆéƒ½å¤±è´¥
            raise RuntimeError(f"æ‰€æœ‰LLMæœåŠ¡éƒ½ä¸å¯ç”¨ã€‚åŸå§‹é”™è¯¯: {error_msg}")
        else:
            # é503é”™è¯¯ï¼Œç›´æ¥æŠ›å‡º
            raise


def resolve_output_path(episode_date: str, output: Optional[str], overwrite: bool) -> Path:
    from path_utils import safe_path
    if output:
        return safe_path(output, Path.cwd())
    year = episode_date[:4]
    target_dir = STAGE3_OUTPUT_DIR / episode_date
    target_dir.mkdir(parents=True, exist_ok=True)
    
    # å¦‚æœä¸è¦†ç›–ï¼Œè‡ªåŠ¨æ‰¾ä¸‹ä¸€ä¸ªå¯ç”¨ç‰ˆæœ¬å·
    if not overwrite:
        version = 1
        while True:
            path = target_dir / f"episode_{episode_date}_v{version}.md"
            if not path.exists():
                return path
            version += 1
            if version > 99:  # é˜²æ­¢æ— é™å¾ªç¯
                raise RuntimeError(f"ç‰ˆæœ¬å·è¶…è¿‡99ï¼Œè¯·æ£€æŸ¥ç›®å½•ï¼š{target_dir}")
    
    return target_dir / f"episode_{episode_date}_v1.md"


def write_output(content: str, path: Path, overwrite: bool, print_only: bool) -> None:
    if print_only:
        print(content)
        return
    if path.exists() and not overwrite:
        raise RuntimeError(f"ç›®æ ‡æ–‡ä»¶å·²å­˜åœ¨ï¼š{path}ï¼ˆä½¿ç”¨ --overwrite è¦†ç›–ï¼‰")
    path.write_text(content, encoding="utf-8")
    print(f"âœ… å·²ç”Ÿæˆè„šæœ¬ï¼š{path}")


def main() -> None:
    args = parse_args()
    prompt_path, episode_date = resolve_prompt_path(args)
    prompt = read_prompt(prompt_path)
    try:
        script = generate_script(args.provider, args.model, prompt, args.temperature, args.max_tokens)
    except Exception as exc:
        sys.stderr.write(f"âŒ è„šæœ¬ç”Ÿæˆå¤±è´¥ï¼š{exc}\n")
        sys.exit(1)

    output_path = resolve_output_path(episode_date, args.output, args.overwrite)
    try:
        write_output(script, output_path, args.overwrite, args.print_only)
    except Exception as exc:
        sys.stderr.write(f"âŒ å†™å…¥å¤±è´¥ï¼š{exc}\n")
        sys.exit(1)


if __name__ == "__main__":
    main()
